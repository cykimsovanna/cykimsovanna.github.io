[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to My Website\nThis is the homepage of my website."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About Me\nHello, I am a student at the University of Texas at Dallas."
  },
  {
    "objectID": "university_work.html",
    "href": "university_work.html",
    "title": "University of Texas at Dallas",
    "section": "",
    "text": "Assignment 1\nAssignment 2\nProject Proposal\n\n\n\n\n\nAssignment 1"
  },
  {
    "objectID": "university_work.html#ms-social-data-analytics-research",
    "href": "university_work.html#ms-social-data-analytics-research",
    "title": "University of Texas at Dallas",
    "section": "",
    "text": "Assignment 1\nAssignment 2\nProject Proposal\n\n\n\n\n\nAssignment 1"
  },
  {
    "objectID": "KM_assignment1.html",
    "href": "KM_assignment1.html",
    "title": "Knowledge Mining Assignment 1",
    "section": "",
    "text": "Reviewing Breiman’s Statistical Modeling: The Two Cultures and Shmueli’s To Explain or to Predict?\nLeo Breiman’s article talks about the two cultures, namely data (stochastic) models and algorithmic (machine learning) models, while Galit Shmueli’s focuses on explanatory and predictive modeling. The two articles follow very similar arguments in that mainstream academia relies too heavily on data models and/or explanatory modeling, which put more emphasis on inference of data. The problem is that people often use such method for prediction as well, which leads to many wrong conclusions. Both authors want to highlight the importance of algorithmic models and/or predictive modeling as they are crucial for making conclusions about the foreseeable future.\nThe main difference between the two is that Breiman strongly advocates for algorithmic models over the traditional data models because he thinks that the latter only leads to irrelevant theory and poor predictive accuracy, whereas Schmueli wants to emphasize the distinction between explanatory and predictive modeling as they serve two different purposes. She does not necessarily believe explanatory modeling is inferior to the other one—it is simply that while it is good in testing hypothesis and understanding causal relationships about past events, it is not useful when trying to generalize future events.\n\n\nBrainstorming Ideas for the Project\n\n1. Will AI affect future employment rates?\n\nData collection: I can look at FRED databases, as well as other research papers that have dealt with this topic.\nMethods: I can use a regression analysis or time series analysis.\nStarting data project: I will first look up the data, and then regress and plot them to predict the trend of whether or not AI will increase unemployment rates in certain sectors of industry.\n\n\n\n2. Will tariffs affect stock market performance?\n\nData collection: Similarly to the first hypothesis, I can look at various datasets that have been published that pertain to the various financial and economic statistics related to the stock market and tariffs.\nMethods: I can also use regression analysis as well as time series analysis for this.\nStarting data project: I will do the same thing as the first one."
  },
  {
    "objectID": "IM_assignment1.html",
    "href": "IM_assignment1.html",
    "title": "Information Management Assignment 1",
    "section": "",
    "text": "1. Applications I have used that employ a database system:\n\nUTD eLearning (Blackboard) – It stores all the materials that are needed for each course that I enroll in and also allows me to submit quizzes and other assignments through it, which would then be accessed and reviewed by my professors.\neBay – In addition to listing all the products that we can buy by just typing the names of those goods or by filtering by certain categories, it also stores our purchase history and other feedback on such products.\nGoogle Drive – It stores files and other user data that we upload onto it. Similarly to how file explorer functions, Google Drive allows us to sort out files by category, date modified, name, etc. and also create folders where we can rearrange those files’ location.\n\n\n\n2. Applications in domain projects:\n\nNews compiler\n\nPurpose – allows users to easily find the news articles they want to read and understand the sources where they come from\nFunctions – compiles all news articles and sorts them by their topics, place of origin, date, as well as perspective; uses AI to determine if the news article is biased towards any political ideology; allows users to see the backgrounds and perspectives of the authors\nSimple interface design – homepage with trending news stories; search-bar where we can look up specific type of news; bias rating for each article\n\nPrice tracker\n\nPurpose – allows users to track any price change of goods and services across different places, from online shops to neighborhood grocery stores\nFunctions – collects real-time pricing data of various products that are sold via online or physical stores; displays historical prices to allow people to see the trends and do comparisons\nSimple interface design – works similarly to eBay and Amazon but contains a sidebar that allows us to see the price changes of specific products and their costs sold at other places\n\nHistory archive\n\nPurpose – makes it accessible to everyone to view any historical sources (literature, art, etc.) of any country or region\nFunctions – compiles all historical sources and sorts them out by location, period, author and genre; gives contexts about each piece of historical record, as well as translations into different languages\nSimple interface design – webpages containing detailed explanation and related pictures for each historical document; bookmark and annotation features that allow users to take notes in case they want to do research and this is fully customisable and personalized\n\n\n\n\n3. Why is data mining needed?\nEven though data can be retrieved efficiently and effectively, data mining is still needed, because to simply put, it is how we process such data to put into use for various purposes, like research and report. We still have to analyze the data, find any pattern or trend, predict future outcomes, and make insights. In other words, while modern database systems allow us to easily get information, data mining is another step of what we do with that information.\n\n\n6. Tables that are used to store information in a social network\n\nUser profile table – contains the user ID, username, bio, email, password, as well as profile picture\nPosts table – contains the post ID, content of the post (texts, audio or video), number of likes and comments, and links that are attached\nComments table – contains the ID, texts, authors’ username, time-stamp of creation, and other replies\nGroup/Community table – contains group name and ID, description and type, creation date, member count, list of moderators that have access to manage the group, types of posts (video, picture, essay, etc.)"
  },
  {
    "objectID": "index.html#projects-and-research",
    "href": "index.html#projects-and-research",
    "title": "Home",
    "section": "",
    "text": "Here is a quick overview of my work and research at university."
  },
  {
    "objectID": "KM_projectproposal.html",
    "href": "KM_projectproposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Brainstorming Ideas\n\n1. Will AI affect future employment rates?\n\nData collection:\nMethods:\nStarting data project:\n\n\n\n2. Will tariffs affect stock market performance?\n\nData collection:\nMethods:\nStarting data project:"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # length() tells us how many values is in each vector (x) or (y)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # This function basically removes all values\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # byrow=F means that the matrix is sorted by column\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # This computes the square root of (x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # rnorm(50,mean=50,sd=.1) generates a vector of 50 numbers with a mean of 50 and standard deviation of 0.1\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9956349\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(10) # Trying different seeds\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] -0.1365489\n\nvar(y)\n\n[1] 0.885925\n\nsqrt(var(y))\n\n[1] 0.9412359\n\nsd(y)\n\n[1] 0.9412359\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"lightgreen\") # Trying different colors\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "length(x)  # length() tells us how many values is in each vector (x) or (y)\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # This function basically removes all values"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # byrow=F means that the matrix is sorted by column\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # This computes the square root of (x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # rnorm(50,mean=50,sd=.1) generates a vector of 50 numbers with a mean of 50 and standard deviation of 0.1\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9956349\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(10) # Trying different seeds\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "mean(y)\n\n[1] -0.1365489\n\nvar(y)\n\n[1] 0.885925\n\nsqrt(var(y))\n\n[1] 0.9412359\n\nsd(y)\n\n[1] 0.9412359"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R Programming Basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"lightgreen\") # Trying different colors\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # -c() removes rows 1 and 3\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\n# Running each function to test which reads data faster\ntime1 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\nproc.time()-time1\n\n   user  system elapsed \n   0.00    0.00    0.06 \n\ntime2 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nproc.time()-time2\n\n   user  system elapsed \n   0.00    0.00    0.05 \n\ntime3 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\")\nproc.time()-time3\n\n   user  system elapsed \n   0.00    0.00    0.06 \n\n# It turns out the result varies a lot after multiple test. However, the third function reads the data quickest for the most time, followed by the second function, and then the first one.\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # The difference is that the null values are removed from that row\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(\"MASS\", \"ISLR\", repos='http://cran.us.r-project.org')\n\nThe following package(s) will be installed:\n- ISLR [1.4]\n- MASS [7.3-64]\nThese packages will be installed into \"C:/Users/chakr/OneDrive/Github Website/cykimsovanna.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing MASS ...                           OK [linked from cache]\n- Installing ISLR ...                           OK [linked from cache]\nSuccessfully installed 2 packages in 16 milliseconds.\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\nfix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# The difference between \"confidence\" and \"prediction\" is that the first shows the range where we expect the mean value to be, while the latter shows the range where we expect a single new value to be.\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit) \n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # -c() removes rows 1 and 3\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "# Running each function to test which reads data faster\ntime1 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\nproc.time()-time1\n\n   user  system elapsed \n   0.00    0.00    0.06 \n\ntime2 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nproc.time()-time2\n\n   user  system elapsed \n   0.00    0.00    0.05 \n\ntime3 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\")\nproc.time()-time3\n\n   user  system elapsed \n   0.00    0.00    0.06 \n\n# It turns out the result varies a lot after multiple test. However, the third function reads the data quickest for the most time, followed by the second function, and then the first one.\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # The difference is that the null values are removed from that row\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(\"MASS\", \"ISLR\", repos='http://cran.us.r-project.org')\n\nThe following package(s) will be installed:\n- ISLR [1.4]\n- MASS [7.3-64]\nThese packages will be installed into \"C:/Users/chakr/OneDrive/Github Website/cykimsovanna.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing MASS ...                           OK [linked from cache]\n- Installing ISLR ...                           OK [linked from cache]\nSuccessfully installed 2 packages in 16 milliseconds.\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\nfix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# The difference between \"confidence\" and \"prediction\" is that the first shows the range where we expect the mean value to be, while the latter shows the range where we expect a single new value to be.\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit) \n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R Programming Basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "KM_assignment2.html",
    "href": "KM_assignment2.html",
    "title": "Knowledge Mining Assignment 2",
    "section": "",
    "text": "1. Practicing Basic R Programming\n\nLab01\nLab02\n\n\n\n2. Running an EDA on Taiwan 2016 Election\n\nlibrary(haven) \nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nattach(TEDS_2016)\n\n\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)), fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steelblue\",\"forestgreen\", \"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\n\n\n\n\n\n\n2.1. What problems did I encounter when working with the dataset?\n\nThe only problems I ran into when running the EDA was that copying and pasting from the Assignment 2 sheet does not work, so I had to do so directly from the website.\n\n\n\n2.2. How to deal with null values?\n\nI would have to use the bottom codes to get rid of them. I am unsure if all values in “NA” should be considered as null values, so I just leave them be. I also remade the graph with the new version.\n\n\nunique(TEDS_2016$PartyID)\n\n[1] NA  KMT DPP NPP NP  PFP TSU\nLevels: KMT DPP NP PFP TSU NPP NA\n\nsummary(TEDS_2016$PartyID)\n\nKMT DPP  NP PFP TSU NPP  NA \n388 591   3  32   5  43 628 \n\nTEDS_2016 &lt;- na.omit(TEDS_2016)\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\n\n\n\n\n\n\n\n2.3. Exploring the relationship between Tondu and other variables\n\nFinding Correlations\n\ncor(TEDS_2016$Tondu, TEDS_2016$age) \n\n[1] 0.004867868\n\ncor(TEDS_2016$Tondu, TEDS_2016$income)\n\n[1] -0.1141681\n\ncor(TEDS_2016$Tondu, TEDS_2016$edu) \n\n[1] -0.1121728\n\ncor(TEDS_2016$Tondu, TEDS_2016$Econ_worse)\n\n[1] 0.02502642\n\n# Because there are some missing values here, I had to make some changes to the code.\ncor(TEDS_2016$Tondu, TEDS_2016$age, use=\"complete.obs\") \n\n[1] 0.004867868\n\ncor(TEDS_2016$Tondu, TEDS_2016$income, use=\"complete.obs\")\n\n[1] -0.1141681\n\ncor(TEDS_2016$Tondu, TEDS_2016$edu, use=\"complete.obs\") \n\n[1] -0.1121728\n\ncor(TEDS_2016$Tondu, TEDS_2016$Econ_worse, use=\"complete.obs\")\n\n[1] 0.02502642\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrelation_heatmap &lt;- TEDS_2016[, c(\"Tondu\", \"age\", \"income\", \"edu\", \"Econ_worse\")]\ncorr_matrix &lt;- cor(correlation_heatmap, use = \"complete.obs\")\ncorrplot(corr_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\n\n\nRegressing Tondu\n\nlm_tondu &lt;- lm(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\nsummary(lm_tondu)\n\n\nCall:\nlm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4565 -1.0357 -0.0931  0.7639  5.8369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.5696493  0.2952361  12.091  &lt; 2e-16 ***\nfemale       0.2033016  0.0847726   2.398   0.0166 *  \nDPP          0.3935885  0.0927837   4.242 2.41e-05 ***\nage         -0.0003988  0.0034532  -0.115   0.9081    \nincome      -0.0351503  0.0150094  -2.342   0.0194 *  \nedu         -0.0641217  0.0393935  -1.628   0.1039    \nTaiwanese    0.9346107  0.0962575   9.709  &lt; 2e-16 ***\nEcon_worse  -0.1535299  0.0872766  -1.759   0.0788 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.363 on 1066 degrees of freedom\nMultiple R-squared:  0.163, Adjusted R-squared:  0.1575 \nF-statistic: 29.66 on 7 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n2.4. Exploring the relationship between votesai and other variables\n\nggplot(TEDS_2016, aes(x = votetsai)) +\n  geom_bar() +\n  labs(title = \"Vote for Tsai Ing-wen\", x = \"Vote Tsai Ing-wen\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(TEDS_2016, aes(x = votetsai, y = age)) +\n  geom_boxplot() +\n  labs(title = \"Age vs. Vote for Tsai Ing-wen\", x = \"Vote Tsai Ing-wen\", y = \"Age\")\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\n\n\n\ncor(TEDS_2016$income, as.numeric(TEDS_2016$votetsai))\n\n[1] -0.05985151\n\nglm_model_tsai &lt;- glm(votetsai ~ age + female + income + edu + Taiwanese + Econ_worse, data = TEDS_2016, family = binomial)\n\nsummary(glm_model_tsai)\n\n\nCall:\nglm(formula = votetsai ~ age + female + income + edu + Taiwanese + \n    Econ_worse, family = binomial, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.158031   0.502036   0.315   0.7529    \nage         -0.014903   0.005964  -2.499   0.0125 *  \nfemale      -0.297665   0.148531  -2.004   0.0451 *  \nincome      -0.011365   0.026004  -0.437   0.6621    \nedu         -0.126866   0.068413  -1.854   0.0637 .  \nTaiwanese    2.026248   0.150262  13.485  &lt; 2e-16 ***\nEcon_worse   0.801789   0.147300   5.443 5.23e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1435.7  on 1073  degrees of freedom\nResidual deviance: 1157.5  on 1067  degrees of freedom\nAIC: 1171.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n2.4. Generating frequency table and barchart of the Tondu variable\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, \n“Status quo, unif. in future”, “Status quo, decide later\", \"Status quo \nforever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 18 131 351 200 270  73  31 \n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(x = Tondu)) + geom_bar(fill = \"blue\") + labs(title = \"Distribution of Tondu Preferences\", x = \"Tondu\", y = \"Frequency\")"
  },
  {
    "objectID": "UT_Dallas.html",
    "href": "UT_Dallas.html",
    "title": "University of Texas at Dallas",
    "section": "",
    "text": "Assignment 1 - 01/28/2025\nAssignment 2 - 02/11/2025\nAssignment 3 - 03/04/2025\nKnowledge Mining Project Proposal - 02/25/2025\nClass 4 Discussion - 02/25/2025\n\n\n\n\n\nAssignment 1 - 02/06/2025\nAssignment 2 - 02/13/2025\nAssignment 3 - 02/20/2025\nAssignment 4 - 02/27/2025\nAssignment 5 - 03/06/2025\nDatabase Project Proposal - 02/20/2025"
  },
  {
    "objectID": "UT_Dallas.html#ms-social-data-analytics-research",
    "href": "UT_Dallas.html#ms-social-data-analytics-research",
    "title": "University of Texas at Dallas",
    "section": "",
    "text": "Assignment 1 - 01/28/2025\nAssignment 2 - 02/11/2025\nAssignment 3 - 03/04/2025\nKnowledge Mining Project Proposal - 02/25/2025\nClass 4 Discussion - 02/25/2025\n\n\n\n\n\nAssignment 1 - 02/06/2025\nAssignment 2 - 02/13/2025\nAssignment 3 - 02/20/2025\nAssignment 4 - 02/27/2025\nAssignment 5 - 03/06/2025\nDatabase Project Proposal - 02/20/2025"
  },
  {
    "objectID": "IM_assignment2.html",
    "href": "IM_assignment2.html",
    "title": "Information Management Assignment 2",
    "section": "",
    "text": "1. Explaining Key Terms:\n\nA relation schema is a logical structure or blueprint of a database that includes definitions for the all the relations within it, such as names, primary and foreign keys, constraints, and domains. For example, within the UTD database, “student” and “course” could be considered as relation schemas:\n\nstudent (studentID, degree, graduation year, school)\ncourse (courseID, coursename, department, credits, class level)\n\nA relation is simply the table within a database that consists of tuples and attributes. It is unordered but its structure is defined by the relation schema. For example, a relation could be:\n\n\n\nCourseID\nCourse Name\nDepartment\n\n\n\n\nEPPS 6323\nKnowledge Mining\nEPPS\n\n\nEPPS 6354\nInformation Management\nEPPS\n\n\nECON 6357\nMonetary Economics & International Banking\nEPPS\n\n\n\nAn instance is a snapshot of the actual data in the database at a specific point in time. An example of this would just be the exact same thing as shown in the table above, except that it is time-specific, which in this case would be from the Spring 2025 semester. An instance from the previous Fall 2024 semester would have different classes available.\n\n\n\n2. Schema Diagram for Bank Database\n\n\n\n\n\n3. Identifying Primary and Foreign Keys\n\nAssuming that branch names and customer names are unique, but loans and accounts can be associated with more than one customer, the appropriate primary keys are underlined in the diagram above, which include ID and customer_name in “customer,” ID in “borrower,” loan_number in “loan,” branch_name in “branch,” account_number in “account,” and ID in “depositor.”\nThe appropriate foreign keys are ID and loan_number in “borrower,” branch_name in “loan,” branch_name in “account,” and ID and account_number in “depositor.”"
  },
  {
    "objectID": "IM_assignment3.html",
    "href": "IM_assignment3.html",
    "title": "Information Management Assignment 3",
    "section": "",
    "text": "Getting a list using SQL codes\n\nStudents IDS\n\nSELECT DISTINCT ID FROM takes\n\n\nInstructors\n\nSELECT name from instructor\n\n\nDepartments\n\nSELECT dept_name FROM department\n\n\n\n\n\nDoing queries using SQL codes\n\nFinding the ID and name of each student who has taken at least 1 Computer Science course (no duplicates)\n\nSELECT DISTINCT s.ID, s.name\nFROM student AS S\nJOIN takes AS t ON s.ID = t.ID\nJOIN course AS c ON t.course_id = c.course_id\nWHERE c.dept_name = 'Comp. Sci.'\n\n\nAdding grades to the list\n\nSELECT DISTINCT s.ID, s.name, t.grade\nFROM student AS S\nJOIN takes AS t ON s.ID = t.ID\nJOIN course AS c ON t.course_id = c.course_id\nWHERE c.dept_name = 'Comp. Sci.'\n\n\nFinding the ID and name of each student who has not taken any course offered before 2017\n\nSELECT s.ID, s.name\nFROM student AS s\nWHERE NOT EXISTS (\n  SELECT 1\n  FROM takes AS t\n  JOIN course AS c ON t.course_ID = c.course_ID\n  WHERE t.ID = s.ID AND t.year &lt; 2017\n)\n\n\nFinding maximum salary of instructors in each department\n\nSELECT dept_name, MAX(salary)\nFROM instructor\nGROUP BY dept_name\n\n\nFinding the lowest, across all departments, of the per-department maximum salary\n\nSELECT dept_name, MAX(salary)\nFROM instructor\nWHERE salary = (\n  SELECT MIN(max_salary)\n  FROM (\n    SELECT MAX(salary) AS max_salary\n    FROM instructor\n    GROUP BY dept_name\n  )\n)\n\n\nAdding names to this list\n\nSELECT dept_name, name, MAX(salary)\nFROM instructor\nWHERE salary = (\n  SELECT MIN(max_salary)\n  FROM (\n    SELECT MAX(salary) AS max_salary\n    FROM instructor\n    GROUP BY dept_name\n  )\n)\n\n\n\n\n\nPracticing more advanced SQL codes\n\nFinding number of students in each section\n\nSELECT t.course_id, t.sec_id, t.year, t.semester, COUNT(t.ID) AS num\nFROM takes AS t\nGROUP BY t.course_id, t.sec_id, t.year, t.semester\nHAVING COUNT(t.ID) &gt; 0\nORDER BY t.course_id, t.sec_id, t.year, t.semester"
  },
  {
    "objectID": "IM_assignment4.html",
    "href": "IM_assignment4.html",
    "title": "Information Management Assignment 4",
    "section": "",
    "text": "1. Explain the difference between a weak and strong entity set\n\nA weak entity set does not have a primary key and must be associated with an identifying entity (or strong entity), and have a total participation in its identifying relationship set. It uses discriminator attributes to help differentiate among weak entities that are tied to the same identifying entity.\nA strong entity set has a primary key that uniquely identifies each entity within the set. It does not depend on other entities and can exist without relationships. Below is an example that shows a strong entity (left) and a weak entity (right).\n\n\n\n\n2. Design an E-R diagram for keeping track of the scoring statistics of my favorite sports team\n\nThe number of goals_scored for every match will be added up to the total_goals_scored for each individual player respectively. Same applies to total_goals_scored, total_fouls_committed, total_yellow_card and total_red_card.\nMeanwhile the goals_scored of each player in the team will be added to either the home_score or away_score.\nThe winner would determine the standing of each team as they gain points.\n\n\n\n\n3. Practice SQL\n\nIn this following query below, appending natural join section in the from clause would not change the result because the section table already contains the same information as in the takes table, since the attributes that we are looking for, like course_id, semester, year and sec_id, are all present in both tables.\n\nselect course_id, semester, year, sec_id, avg (tot_cred)  \nfrom takes natural join student \nwhere year = 2017 \ngroup by course_id, semester, year, sec_id\nhaving count (ID) &gt;= 2\n\n\nTo find the ID of any student who has never taken a course at the university by using no sub-queries and no set operations, I would use the following codes:\n\nselect s.ID, s.name\nfrom student as s\nleft outer join takes t on s.ID = t.ID\nwhere t.course_id is null"
  },
  {
    "objectID": "KM_assignment3_grok1.html",
    "href": "KM_assignment3_grok1.html",
    "title": "Knowledge Mining Assignment 3",
    "section": "",
    "text": "Grok’s Initial Response\nA Systematic Literature Review on the Application of Data Mining and Machine Learning in Real-World Domains\n\nAbstract\nData mining and machine learning (ML) have emerged as transformative tools across diverse real-world domains, from healthcare to finance, agriculture, and cybersecurity. This systematic literature review synthesizes peer-reviewed studies published between 2015 and 2023 to explore how these technologies are applied, identify trends, highlight gaps, and propose future research directions. Through a structured methodology, 52 studies were analyzed, revealing key applications such as predictive diagnostics in healthcare, fraud detection in finance, and yield optimization in agriculture. Trends indicate a growing reliance on deep learning and big data integration, while gaps persist in ethical considerations and interpretability. A testable hypothesis is proposed to address one identified gap.\n\n\n1. Introduction\nData mining and machine learning have revolutionized decision-making by uncovering patterns, predicting outcomes, and automating processes in real-world domains. Data mining involves extracting actionable insights from large datasets, while ML enables systems to learn from data and improve over time without explicit programming (Witten et al., 2016). Together, they address complex challenges across sectors, such as diagnosing diseases, detecting financial fraud, and optimizing supply chains. Despite their widespread adoption, challenges remain in scalability, ethical implications, and domain-specific adaptations. This systematic review aims to:\n\nSynthesize applications of data mining and ML across real-world domains.\nIdentify trends and gaps in current research.\nPropose a testable hypothesis for future investigation.\n\n\n\n2. Methodology\n\n\n2.1 Research Questions\nThis review addresses the following questions:\n\nRQ1: How are data mining and ML applied in real-world domains?\nRQ2: What are the key trends and gaps in these applications?\nRQ3: What future research directions can be proposed?\n\n\n\n2.2 Search Strategy\nA systematic search was conducted across databases such as PubMed, IEEE Xplore, Scopus, and Google Scholar. Keywords included “data mining,” “machine learning,” “real-world applications,” and domain-specific terms (e.g., “healthcare,” “finance,” “agriculture”). Boolean operators (AND, OR) were used to refine searches (e.g., “data mining AND healthcare AND machine learning”). The search was limited to peer-reviewed articles published between 2015 and 2023 to ensure recency and relevance.\n\n\n2.3 Inclusion and Exclusion Criteria\nStudies were included if they: (1) focused on real-world applications of data mining or ML, (2) provided empirical results or case studies, and (3) were published in English. Excluded studies involved purely theoretical models, lacked methodological rigor, or were published before 2015.\n\n\n2.4 Study Selection\nThe search yielded 312 articles. After removing duplicates (n=74), titles and abstracts were screened, excluding 162 studies that did not meet inclusion criteria. Full-text reviews were conducted on 76 papers, with 52 ultimately selected for analysis based on relevance and quality.\n\n\n2.5 Data Extraction and Analysis\nData were extracted on: (1) domain of application, (2) techniques used, (3) key findings, (4) challenges, and (5) future directions. A thematic synthesis approach was adopted to identify patterns, trends, and gaps across studies.\n\n\n3. Results and Synthesis of Key Findings\n\n\n3.1 Applications Across Domains\nHealthcare\nData mining and ML have transformed healthcare through predictive analytics and decision support systems. Studies like Smith et al. (2018) applied ML algorithms (e.g., random forests, neural networks) to electronic health records (EHRs) for early diagnosis of diseases such as diabetes and cancer. For instance, convolutional neural networks (CNNs) achieved 92% accuracy in detecting breast cancer from mammograms (Kumar et al., 2020). Data mining techniques, such as association rule mining, identified comorbidities in patient data, improving treatment plans (Li et al., 2019). Challenges include data privacy (e.g., HIPAA compliance) and model interpretability for clinical adoption.\nFinance\nIn finance, ML models like gradient boosting and deep learning detect fraudulent transactions with high precision. Brown et al. (2021) reported a 95% detection rate for credit card fraud using XGBoost, while clustering techniques identified anomalous market behaviors (Tan et al., 2017). Data mining also supports portfolio optimization by extracting patterns from historical market data. Regulatory compliance and the need for explainable AI remain hurdles (Jones et al., 2022).\nAgriculture\nPrecision agriculture leverages ML for yield prediction and resource optimization. Decision trees and support vector machines (SVMs) predict crop yields with accuracies above 85% by analyzing soil, weather, and historical data (Patel et al., 2020). Data mining techniques like clustering segment farmland for targeted interventions (Gupta et al., 2019). However, limited access to quality data in developing regions hampers scalability (Singh et al., 2023).\nCybersecurity\nML enhances cybersecurity through anomaly detection and threat prediction. Long Short-Term Memory (LSTM) networks identify zero-day malware with 89% accuracy (Chen et al., 2021), while data mining uncovers patterns in network traffic to prevent Distributed Denial-of-Service (DDoS) attacks (Lee et al., 2020). Challenges include adapting to evolving threats and managing false positives.\nOther Domains\nIn transportation, ML optimizes routes and predicts traffic congestion (Wang et al., 2022). In retail, market basket analysis and recommendation systems boost sales through personalized marketing (Nguyen et al., 2018). Education benefits from ML-driven adaptive learning platforms (Miller et al., 2021), though equity in access remains a concern.\n\n\n3.2 Techniques Employed\nCommon ML techniques include supervised algorithms (e.g., SVM, random forests), unsupervised methods (e.g., k-means clustering), and deep learning (e.g., CNNs, LSTMs). Data mining frequently employs association rule mining, clustering, and classification. Hybrid approaches combining ML and data mining, such as integrating clustering with neural networks for feature extraction, are increasingly prevalent (Zhang et al., 2022).\n\n\n3.3 Challenges Across Domains\nRecurring challenges include:\n\nData Quality and Availability: Incomplete or noisy datasets hinder model performance (Kumar et al., 2020).\nScalability: Algorithms struggle with massive, real-time data in domains like finance and cybersecurity (Brown et al., 2021).\nInterpretability: Black-box models like deep learning lack explainability, limiting trust in healthcare and finance (Smith et al., 2018).\nEthical Concerns: Bias in training data perpetuates inequities, as seen in healthcare diagnostics (Li et al., 2019).\n\n\n\n4. Trends and Gaps\n\n\n4.1 Key Trends\n\nRise of Deep Learning: Deep learning dominates applications requiring unstructured data processing, such as image recognition in healthcare and natural language processing in education (Kumar et al., 2020; Miller et al., 2021).\nBig Data Integration: The integration of big data platforms (e.g., Hadoop, Spark) with ML enables real-time analytics, particularly in finance and cybersecurity (Chen et al., 2021).\nAutomation and IoT: IoT devices paired with ML automate processes in agriculture and transportation, enhancing efficiency (Patel et al., 2020; Wang et al., 2022).\nFederated Learning: Emerging in healthcare, federated learning ensures data privacy by training models locally across decentralized datasets (Jones et al., 2022).\n\n\n\n4.2 Research Gaps\n\nEthical Frameworks: Few studies address bias mitigation or ethical implications, especially in domains impacting marginalized groups (Singh et al., 2023).\nInterpretability: While explainable AI (XAI) is gaining attention, its application in critical domains like healthcare remains underexplored (Smith et al., 2018).\nGeneralizability: Models often perform well in controlled settings but fail in diverse, real-world scenarios (Li et al., 2019).\nResource Constraints: Developing regions lack infrastructure for widespread ML adoption in agriculture and education (Gupta et al., 2019).\n\n\n\n5. Discussion\nThe reviewed studies highlight the transformative potential of data mining and ML across domains. In healthcare, predictive models save lives by enabling early interventions, while in finance and cybersecurity, they safeguard systems against threats. Agriculture and transportation benefit from optimization and automation, underscoring ML’s versatility. However, challenges like data quality, scalability, and interpretability temper these benefits. Trends such as deep learning and big data integration signal a move toward more complex, data-driven solutions, yet ethical concerns and generalizability gaps demand attention.\nA notable gap is the lack of robust ethical frameworks to address bias, particularly in healthcare and education, where biased models can exacerbate inequalities. Interpretability also remains a barrier to adoption in high-stakes domains. While federated learning offers promise for privacy-preserving applications, its implementation is nascent and requires further exploration.\n\n\n6. Proposed Testable Hypothesis\nTo address the gap in ethical frameworks, this review proposes the following hypothesis: “The integration of fairness-aware ML algorithms with data mining techniques will reduce algorithmic bias by at least 20% in predictive healthcare models targeting underserved populations, as measured by fairness metrics like equal opportunity difference.” This hypothesis can be tested by developing fairness-aware models (e.g., using adversarial debiasing) and comparing their performance against baseline models on datasets representing diverse demographics.\n\n\n7. Conclusion\nThis systematic review demonstrates that data mining and ML are pivotal in addressing real-world challenges across healthcare, finance, agriculture, cybersecurity, and beyond. Key findings underscore their efficacy in predictive analytics, automation, and optimization, while trends like deep learning and big data integration herald future advancements. However, gaps in ethics, interpretability, and generalizability highlight areas for improvement. The proposed hypothesis offers a starting point for addressing algorithmic bias, paving the way for more equitable applications. Future research should prioritize interdisciplinary collaboration to tackle these challenges and ensure responsible deployment of these technologies.\n\n\nReferences\n(Note: Placeholder references are used due to platform constraints. In a full academic paper, these would be replaced with real citations.)\n\nBrown et al. (2021). “Fraud Detection Using XGBoost.” Journal of Finance Tech.\nChen et al. (2021). “LSTM for Zero-Day Malware Detection.” Cybersecurity Review.\nGupta et al. (2019). “Clustering for Precision Agriculture.” Agricultural Informatics.\nJones et al. (2022). “Explainable AI in Finance.” Journal of AI Ethics.\nKumar et al. (2020). “CNNs for Breast Cancer Detection.” Medical Imaging Journal.\nLee et al. (2020). “Data Mining for DDoS Prevention.” Network Security.\nLi et al. (2019). “Association Rules in Healthcare.” Health Data Science.\nMiller et al. (2021). “ML in Adaptive Learning.” Education Technology.\nNguyen et al. (2018). “Recommendation Systems in Retail.” Marketing Analytics.\nPatel et al. (2020). “Yield Prediction Using Decision Trees.” Precision Agriculture.\nSingh et al. (2023). “Challenges in Agricultural ML Adoption.” Global Farming Studies.\nSmith et al. (2018). “Predictive Analytics in EHRs.” Journal of Medical Systems.\nTan et al. (2017). “Clustering for Market Anomaly Detection.” Finance Analytics.\nWang et al. (2022). “Traffic Prediction with ML.” Transportation Research.\nWitten et al. (2016). Data Mining: Practical Machine Learning Tools and Techniques.\nZhang et al. (2022). “Hybrid ML and Data Mining Approaches.” Data Science Review.\n\n\n\n\nGrok’s Revised Response\nA Systematic Literature Review on the Application of Data Mining and Machine Learning in Real-World Domains\n\nAbstract\nData mining and machine learning (ML) have become indispensable tools for addressing complex challenges across diverse real-world domains, including healthcare, finance, agriculture, and cybersecurity. This systematic literature review synthesizes peer-reviewed studies and recent developments from 2018 to 2025 to explore their applications, identify trends and gaps, and propose future research directions. Following a structured methodology, 48 studies were analyzed, revealing applications like predictive diagnostics, fraud detection, and resource optimization. Trends include the rise of deep learning and federated learning, while gaps persist in ethics and interpretability. A testable hypothesis is proposed to address one identified gap.\n\n\n1. Introduction\nData mining extracts actionable insights from large datasets, while machine learning enables systems to learn and adapt without explicit programming (Witten et al., 2016). Together, they tackle real-world challenges—predicting disease outbreaks, detecting financial fraud, optimizing crop yields, and securing digital infrastructure. Despite their transformative potential, issues like scalability, ethical implications, and domain-specific adaptations remain. This review aims to:\n\nSynthesize applications of data mining and ML across real-world domains.\nIdentify trends and gaps, incorporating recent developments.\nPropose a testable hypothesis for future research.\n\n\n\n2. Methodology\n\n\n2.1 Research Questions\nThis review addresses:\n\nRQ1: How are data mining and ML applied in real-world domains?\nRQ2: What are the key trends and gaps in these applications?\nRQ3: What future research directions can be proposed?\n\n\n\n2.2 Search Strategy\nA systematic search was conducted across databases including PubMed, IEEE Xplore, SpringerLink, and Google Scholar. Keywords included “data mining,” “machine learning,” “real-world applications,” and domain-specific terms (e.g., “healthcare,” “finance”). Boolean operators refined searches (e.g., “data mining AND machine learning AND healthcare”). The search spanned peer-reviewed articles from 2018 to 2025, supplemented by recent news and reports identified via Google News and industry publications to capture ongoing developments.\n\n\n2.3 Inclusion and Exclusion Criteria\nStudies were included if they: (1) focused on real-world applications of data mining or ML, (2) provided empirical results or case studies, and (3) were published in English. Excluded studies lacked methodological rigor, were purely theoretical, or predated 2018. News reports were included if they highlighted practical applications or challenges reported in 2023–2025.\n\n\n2.4 Study Selection\nThe search yielded 287 articles. After removing duplicates (n=63), titles and abstracts were screened, excluding 141 studies. Full-text reviews of 83 papers resulted in 48 selected studies. Additionally, 12 recent news reports were incorporated for trend analysis.\n\n\n2.5 Data Extraction and Analysis\nData were extracted on: (1) domain of application, (2) techniques used, (3) findings, (4) challenges, and (5) future directions. A thematic synthesis approach identified patterns, trends, and gaps.\n\n\n3. Results and Synthesis of Key Findings\n\n\n3.1 Applications Across Domains\nHealthcare\nData mining and ML have revolutionized healthcare through predictive analytics and diagnostics. Obermeyer et al. (2019) applied ML to electronic health records (EHRs), achieving high accuracy in predicting cardiovascular risks using gradient boosting. Deep learning models, such as convolutional neural networks (CNNs), have excelled in medical imaging—Esteva et al. (2017) reported 91% accuracy in skin cancer detection. Data mining techniques like association rule mining identified patient comorbidities, enhancing treatment personalization (Chen et al., 2020). Challenges include data privacy and model interpretability.\nFinance\nIn finance, ML detects fraud and optimizes trading. A 2023 study by Kumar et al. used XGBoost to achieve a 94% detection rate for credit card fraud, leveraging transactional data (Kumar et al., 2023). Data mining facilitates market basket analysis for customer segmentation (Tan et al., 2019). Recent reports highlight ML’s role in real-time fraud detection at institutions like JPMorgan Chase, though regulatory compliance remains a hurdle (Finextra, 2024).\nAgriculture\nPrecision agriculture employs ML for yield prediction and resource management. Support vector machines (SVMs) and decision trees predicted crop yields with 87% accuracy using soil and weather data (Patel et al., 2021). Clustering techniques segmented farmland for irrigation optimization (Gupta et al., 2020). A 2024 FAO report noted ML-driven drones improving pest control in India, though data access in developing regions remains limited (FAO, 2024).\nCybersecurity\nML enhances cybersecurity through anomaly detection. Kim et al. (2022) used Long Short-Term Memory (LSTM) networks to detect zero-day malware with 90% accuracy. Data mining uncovers patterns in network traffic to prevent DDoS attacks (Lee et al., 2021). Recent incidents, like the 2025 CrowdStrike outage, underscore the need for robust models to handle evolving threats (TechCrunch, 2025).\nOther Domains\nIn transportation, ML optimizes logistics—Uber uses ML to predict demand and reduce wait times (Uber Engineering, 2023). In retail, Amazon’s recommendation systems leverage collaborative filtering, boosting sales by 35% (Amazon Science, 2024). Education benefits from ML-driven personalized learning, though equitable access remains a challenge (Miller et al., 2022).\n\n\n3.2 Techniques Employed\nSupervised learning (e.g., SVM, random forests), unsupervised learning (e.g., k-means clustering), and deep learning (e.g., CNNs, LSTMs) dominate ML applications. Data mining frequently uses classification, clustering, and association rule mining. Hybrid approaches, such as combining clustering with neural networks, enhance feature extraction (Zhang et al., 2023).\n\n\n3.3 Challenges Across Domains\n\nData Quality: Noisy or incomplete data reduces model accuracy (Obermeyer et al., 2019).\nScalability: Real-time processing struggles in finance and cybersecurity (Kim et al., 2022).\nInterpretability: Black-box models hinder trust in healthcare and finance (Esteva et al., 2017).\nEthics: Bias in datasets perpetuates inequities, notably in healthcare (Chen et al., 2020).\n\n\n\n4. Trends and Gaps\n\n\n4.1 Key Trends\n\nDeep Learning Dominance: Deep learning excels in unstructured data tasks, such as medical imaging and natural language processing (Esteva et al., 2017). A 2024 Nature article highlighted its role in drug discovery, accelerating development timelines (Nature, 2024).\nFederated Learning: Privacy-preserving federated learning gains traction in healthcare, enabling decentralized model training (Rieke et al., 2020). Google’s 2025 healthcare partnerships showcase its scalability (Google Health, 2025).\nBig Data Integration: Platforms like Apache Spark integrate with ML for real-time analytics in finance and cybersecurity (Kumar et al., 2023).\nSustainability Focus: ML optimizes energy use in agriculture and transportation, aligning with global sustainability goals (FAO, 2024).\n\n\n\n4.2 Research Gaps\n\nEthical Frameworks: Few studies address bias mitigation, especially in healthcare and education (Chen et al., 2020). Reports of biased hiring algorithms in 2024 underscore this gap (MIT Technology Review, 2024).\nInterpretability: Explainable AI (XAI) remains underdeveloped in critical domains (Esteva et al., 2017).\nGeneralizability: Models often fail in diverse settings, as seen in agricultural applications in developing regions (Gupta et al., 2020).\nRegulatory Challenges: Finance and healthcare face stringent regulations, slowing adoption (Finextra, 2024).\n\n\n\n5. Discussion\nData mining and ML demonstrate immense potential across domains. In healthcare, predictive models enable early interventions; in finance, they safeguard transactions; in agriculture, they boost efficiency. Trends like deep learning and federated learning signal a shift toward more sophisticated, privacy-conscious solutions. However, challenges—data quality, interpretability, and ethics—limit broader impact. Recent developments, such as biased AI hiring tools and cybersecurity failures, highlight the urgency of addressing these gaps. While sustainability applications show promise, equitable access and regulatory alignment remain critical.\n\n\n6. Proposed Testable Hypothesis\nTo address the ethics gap, I propose: “Implementing fairness-aware ML algorithms alongside data mining techniques will reduce algorithmic bias by at least 15% in healthcare diagnostics for underserved populations, as measured by fairness metrics like demographic parity.” This can be tested by applying fairness constraints (e.g., reweighting) to models trained on diverse EHR datasets and comparing outcomes with baseline models.\n\n\n7. Conclusion\nThis review underscores the transformative role of data mining and ML across real-world domains while highlighting persistent challenges. Trends like deep learning and federated learning offer exciting prospects, but gaps in ethics, interpretability, and generalizability require attention. The proposed hypothesis provides a pathway toward equitable applications. Future research should prioritize interdisciplinary collaboration to address these gaps and ensure responsible deployment.\n\n\nReferences\n\nAmazon Science. (2024). How Recommendation Systems Drive Sales. [Online report].\nChen et al. (2020). “Association Rule Mining in Healthcare.” Journal of Medical Systems.\nEsteva et al. (2017). “Dermatologist-level Classification of Skin Cancer with Deep Neural Networks.” Nature.\nFAO. (2024). AI and Drones in Precision Agriculture: A Global Review.\nFinextra. (2024). JPMorgan Chase Enhances Fraud Detection with Machine Learning.\nGoogle Health. (2025). Federated Learning in Healthcare Partnerships.\nGupta et al. (2020). “Clustering for Precision Agriculture.” Agricultural Informatics.\nKim et al. (2022). “LSTM for Zero-Day Malware Detection.” Cybersecurity.\nKumar et al. (2023). “Fraud Detection Using XGBoost.” Finance Technology.\nLee et al. (2021). “Data Mining for DDoS Prevention.” Network Security.\nMiller et al. (2022). “ML in Adaptive Learning.” Education Technology.\nMIT Technology Review. (2024). The Hidden Bias in AI Hiring Tools.\nNature. (2024). Deep Learning Accelerates Drug Discovery.\nObermeyer et al. (2019). “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science.\nPatel et al. (2021). “Yield Prediction Using Decision Trees.” Precision Agriculture.\nRieke et al. (2020). “The Future of Digital Health with Federated Learning.” NPJ Digital Medicine.\nTan et al. (2019). “Market Basket Analysis in Finance.” Data Science Journal.\nTechCrunch. (2025). CrowdStrike Outage Highlights AI Security Needs.\nUber Engineering. (2023). Machine Learning for Demand Prediction.\nWitten et al. (2016). Data Mining: Practical Machine Learning Tools.\nZhang et al. (2023). “Hybrid ML and Data Mining Approaches.” Data Science Review.\n\n\n\n\nGrok’s Final Response\nSystematic Literature Review: Applications of Data Mining and Machine Learning in Real-World Domains\n\nAbstract\nThis systematic literature review (SLR) examines the applications of data mining and machine learning (ML) across real-world domains, synthesizing peer-reviewed studies (2015–2025) and recent global developments. Following PRISMA guidelines, 50 studies were analyzed to explore applications, identify trends, highlight gaps, and propose a testable hypothesis. Domains such as healthcare, finance, agriculture, cybersecurity, retail, education, and manufacturing reveal transformative uses like predictive diagnostics and fraud detection. Trends include deep learning advancements and IoT integration, while gaps in data privacy, interpretability, and equitable access persist. A hypothesis targeting privacy concerns is proposed to guide future research toward ethical, scalable solutions.\n\n\n1. Introduction\nThe exponential growth of data has propelled data mining and machine learning (ML) into pivotal roles across industries, enabling pattern recognition, predictive analytics, and automation (Witten et al., 2016). These technologies address complex challenges in healthcare (e.g., disease prediction), finance (e.g., fraud detection), agriculture (e.g., precision farming), and beyond. Despite their impact, issues like ethical concerns, interpretability, and scalability remain. This review systematically evaluates literature and recent developments to:\n\nInvestigate how data mining and ML are applied in real-world domains.\nIdentify prevailing trends and research gaps.\nPropose a testable hypothesis for future investigation.\n\n\n\n2. Methodology\nThis study adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, combining the strongest elements from ChatGPT’s methodology with enhancements for rigor and transparency.\n\n\n2.1 Research Questions\n\nRQ1: How are data mining and ML applied across real-world domains?\nRQ2: What trends and gaps exist in these applications?\nRQ3: What future research directions can be identified?\n\n\n\n2.2 Search Strategy and Inclusion Criteria\nA comprehensive search was conducted across databases including PubMed, IEEE Xplore, Scopus, ACM Digital Library, SpringerLink, and Google Scholar, as adopted from ChatGPT’s approach for breadth. Keywords included “data mining,” “machine learning,” “real-world applications,” and domain-specific terms (e.g., “healthcare,” “finance”). Boolean operators (e.g., “data mining AND machine learning AND agriculture”) refined searches. Inclusion criteria were:\n\nPeer-reviewed journal articles and conference papers (2015–2025).\nStudies focusing on real-world applications of data mining and ML.\nEmpirical or case-study-based studies in English. Additionally, recent news from sources like The Australian, Business Insider, and TechCrunch was reviewed to capture developments (2023–2025), as in my and ChatGPT’s drafts.\n\n\n\n2.3 Study Selection\nThe search yielded 320 articles. After deduplication (n=70), 250 remained. Title/abstract screening excluded 150, and full-text review of 100 resulted in 50 studies for analysis, aligning with my draft’s systematic approach. Ten news reports were also included for trend analysis.\n\n\n2.4 Data Extraction and Analysis\nData were extracted on: (1) application domain, (2) techniques used, (3) findings, (4) challenges, and (5) trends/gaps, as in Copilot’s methodology. A thematic synthesis approach, inspired by ChatGPT, aggregated findings. News reports were analyzed to contextualize academic findings with real-world developments.\n\n\n3. Findings and Synthesis\n\n\n3.1 Applications Across Domains\nDrawing on the strongest findings from all drafts, this section provides detailed examples with verified sources.\nHealthcare\nData mining and ML enhance diagnostics and personalized medicine. Rajkomar et al. (2018) applied deep learning to EHRs, predicting hospital readmissions with 85% accuracy (DOI: 10.1038/s41746-018-0029-1). CNNs achieved 91% accuracy in skin cancer detection (Esteva et al., 2017; DOI: 10.1038/nature21056). Association rule mining identifies drug interactions, improving safety (Chen et al., 2019; DOI: 10.1016/j.jbi.2019.103295).\nFinance\nML detects fraud and informs trading. Dal Pozzolo et al. (2015) used random forests for credit card fraud detection, achieving 93% precision (DOI: 10.1109/SSCI.2015.142). Data mining supports customer segmentation via clustering (Tan et al., 2019). Finextra (2024) reported PayPal’s ML models reducing false positives by 20% (Finextra, 2024; link).\nAgriculture\nPrecision agriculture leverages ML for sustainability. Decision trees predict crop yields with 86% accuracy (Liakos et al., 2018; DOI: 10.3390/s18103274). Clustering optimizes irrigation (Gupta et al., 2020). A 2024 FAO report noted ML-driven pest control in Brazil, but data access in Africa remains limited (FAO, 2024; link).\nCybersecurity\nML strengthens anomaly detection. LSTM networks detect zero-day malware with 89% accuracy (Kim et al., 2016; DOI: 10.1109/TIFS.2016.2584703). Data mining identifies DDoS patterns (Lee et al., 2017). TechCrunch (2025) reported ransomware surges, highlighting the need for adaptive models (TechCrunch, 2025; link).\nRetail and E-commerce\nML powers recommendation systems. Collaborative filtering drives Amazon’s recommendations, boosting sales by 35% (Amazon Science, 2024; link). Data mining segments customers (Nguyen et al., 2021). Business Insider (2024) noted startups automating unstructured data preparation (Business Insider, 2024; link).\nEducation\nML personalizes learning and predicts dropouts. Clustering tailors curricula, improving outcomes by 15% (Borge et al., 2019). Data mining identifies at-risk students (Smith et al., 2020). UNESCO (2024) highlights inequities in access (UNESCO, 2024; link).\nManufacturing\nML optimizes predictive maintenance. Random forests predict equipment failures, reducing downtime by 20% (Li et al., 2019; DOI: 10.1016/j.ress.2019.106571). Data mining detects production defects (Wang et al., 2018).\n\n\n3.2 Key Trends\nCombining trends from all drafts, this section incorporates verified developments:\n\nDeep Learning Advancements: Deep learning excels in complex tasks (Esteva et al., 2017). Nature (2024) reported its role in protein folding (Nature, 2024; link).\nExplainability and Fairness: Focus on interpretable AI grows in healthcare and finance (Ribeiro et al., 2016; DOI: 10.1145/2939672.2939778).\nEmerging Tech Integration: ML converges with IoT and blockchain, e.g., in supply chains (IBM Blockchain, 2024; link).\nReal-Time Analytics: Big data platforms enable real-time processing (Dal Pozzolo et al., 2015).\nSustainability Applications: AI monitors microplastics (The Australian, 2024; link).\n\n\n\n3.3 Recent Developments\nFrom ChatGPT’s draft:\n\nEnvironmental Monitoring: AI surveys ocean microplastics rapidly (The Australian, 2024).\nUnstructured Data: Startups automate data preparation (Business Insider, 2024).\nPublic Policy: ML improves housing assistance allocation (Vox, 2024; link).\nLaw Enforcement: AI builds suspect profiles (The Times, 2024; link).\n\n\n\n3.4 Identified Research Gaps\nCombining gaps from all drafts:\n\nData Privacy: Limited research on privacy-preserving techniques (Dwork et al., 2016; DOI: 10.1561/0400000042).\nInterpretability: Black-box models deter adoption (Ribeiro et al., 2016).\nEquitable Access: Developing regions lack infrastructure (FAO, 2024).\nScalability and Generalizability: Models fail in diverse settings (Liakos et al., 2018).\nStandardized Metrics: Uniform evaluation metrics are lacking (Kim et al., 2016).\n\n\n\n\n4. Proposed Hypothesis\nDrawing from my draft for its focus on privacy and Copilot’s emphasis on emerging tech integration, I propose:\n“The integration of differential privacy with data mining and ML, combined with IoT and blockchain, will reduce privacy breaches by at least 20% in predictive healthcare analytics, as measured by privacy loss metrics, while maintaining model accuracy.”\nThis can be tested by applying differential privacy mechanisms to ML models on healthcare datasets, integrating IoT data, and securing results with blockchain.\n\n5. Conclusion\nThis SLR underscores the transformative role of data mining and ML across healthcare, finance, agriculture, cybersecurity, retail, education, and manufacturing. Trends like deep learning, XAI, and IoT integration signal sophisticated applications, while gaps in privacy, interpretability, and access highlight areas for improvement. Recent developments—microplastic monitoring, housing allocation—reflect real-world relevance. The proposed hypothesis addresses privacy concerns, paving the way for ethical, scalable solutions. Future research should prioritize privacy-preserving techniques, standardized metrics, and inclusive deployment.\n\n\nReferences\n\nBorge et al. (2019). “Personalized Learning through Clustering.” Educational Technology Research.\nBusiness Insider. (2024). Startups Automate Unstructured Data for ML. link.\nChen et al. (2019). “Association Rule Mining in Healthcare.” Journal of Biomedical Informatics. DOI: 10.1016/j.jbi.2019.103295.\nDal Pozzolo et al. (2015). “Credit Card Fraud Detection with Random Forests.” IEEE Symposium on Computational Intelligence. DOI: 10.1109/SSCI.2015.142.\nDwork et al. (2016). “Differential Privacy: A Survey.” Foundations and Trends in Theoretical Computer Science. DOI: 10.1561/0400000042.\nEsteva et al. (2017). “Dermatologist-level Classification of Skin Cancer with Deep Neural Networks.” Nature. DOI: 10.1038/nature21056.\nFAO. (2024). AI in Agriculture: Global Case Studies. link.\nFinextra. (2024). PayPal Reduces False Positives in Fraud Detection with ML. link.\nGulshan et al. (2016). “Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy.” JAMA. DOI: 10.1001/jama.2016.17216.\nIBM Blockchain. (2024). Blockchain and AI in Supply Chain Management. link.\nKim et al. (2016). “LSTM for Zero-Day Malware Detection.” IEEE Transactions on Information Forensics and Security. DOI: 10.1109/TIFS.2016.2584703.\nLee et al. (2017). “Data Mining for DDoS Prevention.” IEEE Network Security.\nLi et al. (2019). “Predictive Maintenance with Random Forests.” Reliability Engineering & System Safety. DOI: 10.1016/j.ress.2019.106571.\nLiakos et al. (2018). “Machine Learning in Agriculture: A Review.” Sensors. DOI: 10.3390/s18103274.\nNature. (2024). Deep Learning in Protein Folding for Drug Discovery. link.\nRajkomar et al. (2018). “Scalable and Accurate Deep Learning with Electronic Health Records.” NPJ Digital Medicine. DOI: 10.1038/s41746-018-0029-1.\nRibeiro et al. (2016). “Why Should I Trust You? Explaining the Predictions of Any Classifier.” KDD. DOI: 10.1145/2939672.2939778.\nTan et al. (2019). “Market Basket Analysis in Finance.” Data Science Journal.\nTechCrunch. (2025). Ransomware Surge Highlights Need for Adaptive AI. link.\nThe Australian. (2024). AI Surveys Ocean Microplastic Contamination. link.\nThe Times. (2024). AI in Policing: Building Suspect Profiles. link.\nUNESCO. (2024). AI in Education: Bridging the Access Gap. link.\nVox. (2024). ML Improves Housing Assistance Allocation. link.\nWitten et al. (2016). Data Mining: Practical Machine Learning Tools. Morgan Kaufmann."
  },
  {
    "objectID": "KM_assignment3.html",
    "href": "KM_assignment3.html",
    "title": "Knowledge Mining Assignment 3",
    "section": "",
    "text": "Step 1. Creating Initial Prompt\n\nI will be using ChatGPT, Grok and Copilot. The following is the prompt for this test.\nConduct a 2,000-word structured systematic literature review on how data mining and machine learning are applied in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards.\nHere are the responses from ChatGPT, Grok and Copilot.\n\n\n\nStep 2. Analyzing Model Responses\n\nFor ChatGPT:\n\nThere is a methodology section, which includes research questions, search criteria and data collection.\nThe findings are rather concise as ChatGPT just lists various applications for each domain in bullet points without explaining them more deeply.\nIt does identify trends and research gaps, but they are very generic in response.\nIt only gives a hypothesis talking how predictive models can improve clinicians’ trust and adoption rates, which may be testable, although it focuses only on one domain.\nIt does not provide any citation.\n\nFor Grok:\n\nSimilar to ChatGPT’s, the methodology section here includes everything, as well as the inclusion/exclusion criteria and study selection.\nIt explains quite briefly into how machine learning affects different domains with some statistics backed by sources. It also lists the techniques employed and challenges that may arise–overall, a big improvement over ChatGPT.\nThe trends and gaps are mostly similar to what ChatGPT writes but contains sources.\nSurprisingly, Grok chooses to focus on healthcare as well, though it talks more about how data mining can reduce algorithmic bias in healthcare models for underserved populations.\nWhile Grok includes sources, all of them appear to be fake because when I search them up, either they get the authors’ name wrong or that they lead to other real articles that have different authors and titles, albeit similar topics. This makes it harder to believe any of the statistics it used.\n\nFor Copilot:\n\nThe methodology is very similar to what the other two models have generated.\nThe findings are the same generic responses that briefly outline how data mining can affect various industries. It does provide sources, but they all hilariously send me to sites talking about how to write a systematic review itself.\nThe trends and gaps are just summaries in bullet points.\nAs for the hypothesis, it talks about healthcare as well.\nFor some reason, some of the sources that are listed at the bottom do link to books that actually talk about machine learning and its real-world applications.\n\n\n\n\nStep 3. Refining the Prompt\n\nSince all of the AI models have similar problems when it comes to listing the relevant sources and giving detailed trends and gaps, here is the revised prompt:\nConduct a 2,000-word structured systematic literature review on the topic about how data mining and machine learning are applied in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps by looking at ongoing news around the world in great detail, and propose one testable hypothesis. Incorporate actual sources that are relevant to the topic. Maintain a rigorous academic tone and emulate systematic review standards.\nThe new responses are linked here (same pages but scroll down): ChatGPT, Grok and Copilot.\nThere are slightly more details into the findings and trends/gaps sections for all three AI responses. ChatGPT’s output is somehow cut off mid-way through talking about research gaps, which could be the limit in how much it can generate. However, this time, ChatGPT manages to link to sources that are indeed relevant to the topic. Copilot uses the same sources, but also adds more. Grok, on the other hand, still utilizes the same fake sources. The ones that it has added also suffer from the same problem. Only a couple actually manages to link to actual relevant sites.\n\n\n\nStep 4. Cross-Model Collaboration\n\nDespite Grok using odd sources for its citations, it still provides relatively the most detailed response. Here is the new prompt for Grok that asks it to synthesize the results.\nUsing these drafts from ChatGPT, Copilot as well as yours, conduct a 2,000-word structured systematic literature review on the topic about how data mining and machine learning are applied in real-world domains. Combine the strongest methodology, findings, trends, gaps and hypothesis into a cohesive academically sound document. Check if the sources are actually real and then incorporate them into the document with proper links.\nHere is the final result (bottom of page).\n\n\n\nStep 5. Reflection\n\nOverall, I think Grok does the writing better than the others although its weakness comes from its use of sources. Even with explicit instruction as seen above in the new synthetic prompt, Grok somehow gives links to random articles even though its citations say otherwise. Copilot is probably the best when it comes to listing sources—at least they are accurate and show what they say. ChatGPT performs the worst of all with the least amount of details. When I give it the revised prompt, ChatGPT gives decent sources, but suffers from what is possibly the word limit that it can generate. What I have learned here is that I have to be very explicit in giving each AI model the prompts."
  },
  {
    "objectID": "KM_assignment3_chatgpt1.html",
    "href": "KM_assignment3_chatgpt1.html",
    "title": "Knowledge Mining Assignment 3",
    "section": "",
    "text": "ChatGPT’s Initial Response\nTitle: Systematic Literature Review on the Application of Data Mining and Machine Learning in Real-World Domains\nAbstract\nThis systematic literature review (SLR) explores the applications of data mining and machine learning (ML) across various real-world domains. The study follows a structured methodology to identify, analyze, and synthesize relevant literature, highlighting key findings, trends, and research gaps. The review concludes with a proposed testable hypothesis for future research.\n1. Introduction\nThe proliferation of data in the modern era has led to increased reliance on data mining and machine learning techniques across multiple industries. These techniques facilitate pattern recognition, predictive analytics, and decision-making automation. This review systematically evaluates literature across different domains, identifies prevailing trends, and outlines areas requiring further research.\n2. Methodology\nThis study adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure rigor and transparency.\n2.1 Research Questions\n\nHow are data mining and ML applied across different real-world domains?\nWhat trends and gaps exist in the application of these technologies?\nWhat potential future research directions can be identified?\n\n2.2 Search Strategy and Inclusion Criteria\nA comprehensive search was conducted using databases such as IEEE Xplore, ACM Digital Library, Scopus, and Google Scholar. The inclusion criteria were:\n\nPeer-reviewed journal articles and conference papers (2015–2024)\nStudies focusing on real-world applications of data mining and ML\nEmpirical and theoretical studies\n\n2.3 Data Extraction and Analysis\nData were extracted based on key themes, methodologies, applications, and identified challenges. A thematic synthesis approach was employed.\n3. Findings and Synthesis\n3.1 Applications Across Domains\n\nHealthcare: Predictive modeling for disease diagnosis, drug discovery, and patient monitoring.\nFinance: Fraud detection, algorithmic trading, and risk assessment.\nManufacturing: Predictive maintenance, quality control, and process optimization.\nRetail and E-commerce: Customer segmentation, recommendation systems, and demand forecasting.\nCybersecurity: Intrusion detection, malware classification, and anomaly detection.\nEducation: Personalized learning, early dropout prediction, and academic performance assessment.\n\n3.2 Key Trends\n\nDeep learning models have gained prominence due to their high accuracy in complex pattern recognition tasks.\nExplainability and fairness in AI remain key concerns, particularly in healthcare and finance.\nThe integration of ML with Internet of Things (IoT) and blockchain technologies is emerging.\n\n3.3 Identified Research Gaps\n\nLimited research on data privacy-preserving ML techniques.\nNeed for more standardized benchmarks for evaluating ML performance across domains.\nUnderexplored applications in developing economies.\n\n4. Proposed Hypothesis\n“Integrating explainable AI techniques in healthcare predictive models improves clinicians’ trust and adoption rates without significantly compromising model accuracy.”\n5. Conclusion\nThis systematic review highlights the widespread impact of data mining and ML in real-world applications, identifies key trends and gaps, and proposes future research directions. Addressing the identified gaps will enhance the effectiveness and ethical deployment of these technologies across domains.\nReferences\n(A comprehensive list of cited works should be included here following a proper citation style such as APA or IEEE.)\n\n\nChatGPT’s Revised Response\nTitle: Systematic Literature Review on the Application of Data Mining and Machine Learning in Real-World Domains\nAbstract\nThis systematic literature review (SLR) investigates the applications of data mining and machine learning (ML) across various real-world domains. Employing a rigorous methodology, the study identifies, analyzes, and synthesizes relevant literature, highlighting key findings, emerging trends, and research gaps. Additionally, it incorporates recent developments from global news sources to provide a contemporary perspective. The review concludes with a proposed testable hypothesis for future research.\n1. Introduction\nThe exponential growth of data in the digital age has propelled the integration of data mining and machine learning techniques across multiple industries. These methodologies facilitate pattern recognition, predictive analytics, and automation in decision-making processes. This review systematically evaluates literature and recent developments across different domains, identifies prevailing trends, and outlines areas necessitating further research.\n2. Methodology\nThis study adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure rigor and transparency.\n2.1 Research Questions\n\nHow are data mining and ML applied across different real-world domains?\nWhat trends and gaps exist in the application of these technologies?\nWhat potential future research directions can be identified?\n\n2.2 Search Strategy and Inclusion Criteria\nA comprehensive search was conducted using databases such as IEEE Xplore, ACM Digital Library, Scopus, and Google Scholar. The inclusion criteria were:\n\nPeer-reviewed journal articles and conference papers (2015–2024)\nStudies focusing on real-world applications of data mining and ML\nEmpirical and theoretical studies\n\n2.3 Data Extraction and Analysis\nData were extracted based on key themes, methodologies, applications, and identified challenges. A thematic synthesis approach was employed. Additionally, recent news articles were analyzed to incorporate the latest developments in the field.\n3. Findings and Synthesis\n3.1 Applications Across Domains\n\nHealthcare:\n\nPredictive Modeling: ML algorithms have been utilized for disease diagnosis and prognosis, enhancing patient outcomes.\nbmcmedinformdecismak.biomedcentral.com\nDrug Discovery: Data mining accelerates the identification of potential drug candidates by analyzing large datasets.\nPatient Monitoring: Wearable devices coupled with ML monitor patient vitals, enabling proactive interventions.\n\nFinance:\n\nFraud Detection: ML models identify anomalies in transaction data, mitigating fraudulent activities.\nAlgorithmic Trading: Data mining informs trading strategies by analyzing market trends.\nRisk Assessment: ML predicts credit risks, aiding in informed lending decisions.\n\nManufacturing:\n\nPredictive Maintenance: ML forecasts equipment failures, reducing downtime.\nQuality Control: Data mining detects defects in production processes, ensuring product quality.\nProcess Optimization: ML algorithms optimize manufacturing workflows for efficiency.\n\nRetail and E-commerce:\n\nCustomer Segmentation: Data mining categorizes customers based on behavior, facilitating targeted marketing.\nRecommendation Systems: ML algorithms suggest products to users, enhancing sales.\nDemand Forecasting: Data mining predicts product demand, optimizing inventory management.\n\nCybersecurity:\n\nIntrusion Detection: ML models identify unauthorized access attempts, bolstering security.\nMalware Classification: Data mining differentiates between benign and malicious software.\nAnomaly Detection: ML detects unusual patterns indicating potential security threats.\n\nEducation:\n\nPersonalized Learning: ML tailors educational content to individual student needs.\nEarly Dropout Prediction: Data mining identifies students at risk of dropping out, enabling timely interventions.\nAcademic Performance Assessment: ML evaluates student performance, guiding instructional strategies.\n\n\n3.2 Key Trends\n\nAdvancements in Deep Learning: Deep learning models have gained prominence due to their high accuracy in complex pattern recognition tasks.\nSpringer Link\nExplainability and Fairness: There is a growing emphasis on making AI models interpretable and ensuring they operate without bias, especially in sensitive areas like healthcare and finance.\nIntegration with Emerging Technologies: The convergence of ML with Internet of Things (IoT) and blockchain technologies is creating new opportunities for innovation.\n\n3.3 Recent Developments and News\n\nEnvironmental Monitoring: Researchers have developed AI technology capable of rapidly surveying ocean microplastic contamination, analyzing thousands of samples in seconds—a task that would take humans months.\ntheaustralian.com.au\nUnstructured Data Processing: Startups are focusing on automating the preparation of unstructured data for ML models, enhancing efficiency and accuracy in data extraction.\nBusiness Insider\nPublic Policy and Resource Allocation: Machine learning is being employed to improve the assessment and prioritization of housing assistance for homeless individuals, aiming for fairer allocation of scarce resources.\nvox.com\nLaw Enforcement: AI technologies are being integrated into policing to aggregate data from multiple sources, creating comprehensive suspect profiles and uncovering patterns that human detectives might miss.\nThe Times\n\n3.4 Identified Research Gaps\n\nData Privacy: There is limited research on techniques that preserve data privacy while utilizing ML, highlighting the need for advancements in this area."
  },
  {
    "objectID": "KM_assignment3_copilot1.html",
    "href": "KM_assignment3_copilot1.html",
    "title": "Knowledge Mining Assignment 3",
    "section": "",
    "text": "Copilot’s Initial Response\n\nSystematic Literature Review: Applications of Data Mining and Machine Learning in Real-World Domains\n\n\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning in various real-world domains. By synthesizing key findings from recent studies, identifying trends and gaps, and proposing a testable hypothesis, this review aims to provide a comprehensive understanding of the current state of research in this field.\n\n\nIntroduction\nData mining and machine learning have revolutionized numerous industries by enabling the extraction of valuable insights from large datasets. These technologies are applied in diverse domains such as healthcare, finance, cybersecurity, and more. This review aims to systematically analyze the existing literature to understand how these technologies are being utilized, identify trends and gaps, and propose a hypothesis for future research.\n\n\nMethodology\nThe methodology for this systematic literature review follows a structured approach to ensure transparency and reproducibility. The steps involved are as follows:\n\nResearch Question: The primary research question guiding this review is: “How are data mining and machine learning applied in real-world domains?”\nSearch Strategy: A comprehensive search was conducted across multiple databases, including PubMed, IEEE Xplore, Google Scholar, and SpringerLink. Keywords used in the search included “data mining,” “machine learning,” “applications,” “real-world domains,” and “systematic review.”\nInclusion and Exclusion Criteria: Studies were included if they focused on the application of data mining and machine learning in real-world domains, were published in peer-reviewed journals, and were written in English. Studies were excluded if they were not relevant to the research question, were not peer-reviewed, or were duplicates.\nData Extraction: Relevant data were extracted from the selected studies, including the domain of application, the specific techniques used, key findings, and any identified trends or gaps.\nSynthesis of Findings: The extracted data were synthesized to provide a comprehensive overview of the applications of data mining and machine learning in real-world domains.\n\n\n\nKey Findings\nThe review identified several key findings across various domains:\n\nHealthcare: Data mining and machine learning are extensively used in healthcare for predictive analytics, disease diagnosis, and personalized treatment plans. Techniques such as decision trees, support vector machines, and neural networks are commonly employed.\nFinance: In the finance sector, these technologies are used for fraud detection, risk management, and algorithmic trading. Machine learning models like random forests and gradient boosting are particularly effective.\nCybersecurity: Data mining and machine learning play a crucial role in detecting and preventing cyber threats. Anomaly detection and clustering techniques are widely used to identify suspicious activities.\nE-commerce: These technologies are applied in e-commerce for customer segmentation, recommendation systems, and sentiment analysis. Collaborative filtering and natural language processing are popular techniques.\nAgriculture: In agriculture, data mining and machine learning are used for crop prediction, soil analysis, and precision farming. Techniques such as k-means clustering and regression analysis are commonly applied.\n\n\n\nTrends and Gaps\nThe review identified several trends and gaps in the literature:\n\nTrends:\n\nIncreasing use of deep learning techniques across various domains.\nGrowing emphasis on real-time data processing and analysis.\nIntegration of data mining and machine learning with other technologies such as IoT and blockchain.\n\nGaps:\n\nLimited research on the ethical implications of data mining and machine learning.\nNeed for more studies on the scalability and generalizability of models.\nLack of standardized evaluation metrics for comparing different techniques.\n\n\n\n\nProposed Hypothesis\nBased on the findings of this review, the following hypothesis is proposed for future research:\n\nHypothesis: “The integration of data mining and machine learning with IoT and blockchain technologies will significantly enhance the accuracy and efficiency of predictive analytics in healthcare.”\n\n\n\nConclusion\n\nThis systematic literature review provides a comprehensive overview of the applications of data mining and machine learning in real-world domains. By synthesizing key findings, identifying trends and gaps, and proposing a testable hypothesis, this review contributes to the understanding of the current state of research in this field and highlights areas for future investigation.\n\nHealthcare Applications, Finance Applications, Cybersecurity Applications, E-commerce Applications\n\n\n\n\n\nCopilot’s Revised Response\n\nSystematic Literature Review: Applications of Data Mining and Machine Learning in Real-World Domains\n\n\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning in various real-world domains. By synthesizing key findings from recent studies, identifying trends and gaps, and proposing a testable hypothesis, this review aims to provide a comprehensive understanding of the current state of research in this field.\n\n\nIntroduction\nData mining and machine learning have revolutionized numerous industries by enabling the extraction of valuable insights from large datasets. These technologies are applied in diverse domains such as healthcare, finance, cybersecurity, and more. This review aims to systematically analyze the existing literature to understand how these technologies are being utilized, identify trends and gaps, and propose a hypothesis for future research.\n\n\nMethodology\nThe methodology for this systematic literature review follows a structured approach to ensure transparency and reproducibility. The steps involved are as follows:\n\nResearch Question: The primary research question guiding this review is: “How are data mining and machine learning applied in real-world domains?”\nSearch Strategy: A comprehensive search was conducted across multiple databases, including PubMed, IEEE Xplore, Google Scholar, and SpringerLink. Keywords used in the search included “data mining,” “machine learning,” “applications,” “real-world domains,” and “systematic review.”\nInclusion and Exclusion Criteria: Studies were included if they focused on the application of data mining and machine learning in real-world domains, were published in peer-reviewed journals, and were written in English. Studies were excluded if they were not relevant to the research question, were not peer-reviewed, or were duplicates.\nData Extraction: Relevant data were extracted from the selected studies, including the domain of application, the specific techniques used, key findings, and any identified trends or gaps.\nSynthesis of Findings: The extracted data were synthesized to provide a comprehensive overview of the applications of data mining and machine learning in real-world domains.\n\n\n\nKey Findings\nThe review identified several key findings across various domains:\n\nHealthcare: Data mining and machine learning are extensively used in healthcare for predictive analytics, disease diagnosis, and personalized treatment plans. Techniques such as decision trees, support vector machines, and neural networks are commonly employed.\nFinance: In the finance sector, these technologies are used for fraud detection, risk management, and algorithmic trading. Machine learning models like random forests and gradient boosting are particularly effective.\nCybersecurity: Data mining and machine learning play a crucial role in detecting and preventing cyber threats. Anomaly detection and clustering techniques are widely used to identify suspicious activities.\nE-commerce: These technologies are applied in e-commerce for customer segmentation, recommendation systems, and sentiment analysis. Collaborative filtering and natural language processing are popular techniques.\nAgriculture: In agriculture, data mining and machine learning are used for crop prediction, soil analysis, and precision farming. Techniques such as k-means clustering and regression analysis are commonly applied.\n\n\n\nTrends and Gaps\nThe review identified several trends and gaps in the literature:\n\nTrends:\n\nIncreasing use of deep learning techniques across various domains.\nGrowing emphasis on real-time data processing and analysis.\nIntegration of data mining and machine learning with other technologies such as IoT and blockchain.\n\nGaps:\n\nLimited research on the ethical implications of data mining and machine learning.\nNeed for more studies on the scalability and generalizability of models.\nLack of standardized evaluation metrics for comparing different techniques.\n\n\n\n\nProposed Hypothesis\nBased on the findings of this review, the following hypothesis is proposed for future research:\n\nHypothesis: “The integration of data mining and machine learning with IoT and blockchain technologies will significantly enhance the accuracy and efficiency of predictive analytics in healthcare.”\n\n\n\nConclusion\nThis systematic literature review provides a comprehensive overview of the applications of data mining and machine learning in real-world domains. By synthesizing key findings, identifying trends and gaps, and proposing a testable hypothesis, this review contributes to the understanding of the current state of research in this field and highlights areas for future investigation."
  },
  {
    "objectID": "IM_assignment5.html",
    "href": "IM_assignment5.html",
    "title": "Information Management Assignment 5",
    "section": "",
    "text": "1. An E-R diagram can be viewed as a graph. What do the following mean in terms of the structure of an enterprise schema?\n\nWhen the graph is disconnected, it means that some entities don’t have any relationship with others so they remain isolated–though still in the same schema.\nWhen the graph has a cycle, it means that there are some redundant relationships that create loops between the entities.\n\n\n\n3. Why do we have weak entity sets when we can convert any weak entity set to a strong entity set by simply adding appropriate attributes?\n\nWeak entity sets still exist because if we only have strong entity sets, then the data can become redundant and potentially more complicated to organize, especially when some relationships are dependent upon another.\n\n\n\n4. Practicing SQL\n\nBased on the following employee database:\nemployee(ID, person_name, street, city)\nworks(ID, company_name, salary)\ncompany(company_name, city)\nmanages(ID, manager_id)\n\nFind ID and name of each employee who lives in the same city as the location of the company for which the employee works\n\nselect e.ID, e.person_name\nfrom employee as e\njoin works as w on e.ID = w.ID\njoin company as c on w.company_name = c.company_name\nwhere e.city = c.city\n\nFind ID and name of each employee who lives in the same city and on the same street as does their manager\n\nselect e.ID, e.person_name\nfrom employee as e\njoin manages as m on e.ID = m.ID\njoin employee as em on m.manager_id = em.ID\nwhere e.city = em.city and e.street = em.street\n\nFind ID and name of each employee who earns more than the average salary of all employees of their company\n\nselect e.ID, e.person_name\nfrom employee as e\njoin works as w on e.ID = w.ID\nwhere w.salary &gt; (\n  select avg(w_avg.salary)\n  from works as w_avg\n  where w.company_name = w_avg.company_name\n)\n\n\nWhat is wrong with the following SQL query?\n\nselect name, title\nfrom instructor natural join teaches natural join section natural join course\nwhere semester = 'Spring' and year = 2017\n\nThat SQL query yields the following result:\n\nThis shows a repeat of one of the rows. After doing some more research, I found out that this is due to that ‘Game Design’ course having two sections.\n\nWhen I simply take out the clause natural join section, it yields the same table as the original query’s. To avoid this issue, I just need to add distinct, as follows:\n\nselect distinct name, title\nfrom instructor natural join teaches natural join course\nwhere semester = 'Spring' and year = 2017"
  }
]